{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8ee5ce",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43753a40",
   "metadata": {},
   "source": [
    "## Download Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bfa842-0087-4366-8286-9a4e59912898",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f11eba-8259-4547-bc61-5e531a4e85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc5d64-787c-4ce7-9588-4944dd00ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/stanfordnlp/glove/resolve/main/glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785b122-bb90-4cbb-8338-e081863b868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58b8dc-7e67-4d62-9a67-f5a1afe28c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be77805-3b2d-4ef2-9682-80fd05849d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58da11",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc65338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d563b3ea",
   "metadata": {},
   "source": [
    "## Global Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec100143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b33390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_embedding_matrix(embeddings, word_index, nb_words, embedding_dim):\n",
    "    # Initialize the embedding matrix as a zeros matrix\n",
    "    embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "    # Fill the embedding matrix\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1b70a",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Load the test labels\n",
    "test_labels_df = pd.read_csv('dataset/test_labels.csv')\n",
    "\n",
    "# Filter out rows in test_labels where all label columns are -1\n",
    "mask = (test_labels_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] == -1).all(axis=1)\n",
    "test_labels_df = test_labels_df[~mask]\n",
    "\n",
    "# Now join test_df and test_labels_df on 'id' to get the final test dataset\n",
    "test_df = pd.merge(test_df, test_labels_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the 'comment_text' column in the train dataset\n",
    "train_text = train_df['comment_text'].astype(str)\n",
    "\n",
    "# Access the 'comment_text' column in the test dataset\n",
    "test_text = test_df['comment_text'].astype(str)\n",
    "\n",
    "# Access the labels in the train dataset\n",
    "train_labels = train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "# Access the labels in the test dataset\n",
    "test_labels = test_labels_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21264c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the text data (both train and test)\n",
    "# This will create the vocabulary index based on word frequency\n",
    "tokenizer.fit_on_texts(pd.concat([train_text, test_text]))\n",
    "\n",
    "# Transform each text in train_text to a sequence of integers with its corresponding integer value from the word_index dictionary\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "\n",
    "# Transform each text in test_text to a sequence of integers with its corresponding integer value from the word_index dictionary\n",
    "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "# Transform each sequence in train_sequences to the same length\n",
    "# This is done by padding sequences that are shorter than the longest sequence, and truncating sequences that are longer\n",
    "train_data = pad_sequences(train_sequences)\n",
    "\n",
    "# Transform each sequence in test_sequences to the same length as the sequences in train_data\n",
    "# This is done by padding or truncating as necessary\n",
    "test_data = pad_sequences(test_sequences, maxlen=train_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eba039",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fb0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_glove_vectors(\"glove.6B.300d.txt\")\n",
    "embedding_dim = 300 \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "embedding_matrix = fill_embedding_matrix(embeddings, word_index, nb_words, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b3806",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embedding_dim, weights=[embedding_matrix]))\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d03e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, train_labels, epochs=2, batch_size=300, validation_split=0.1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "model.save('glove-6B-300d-Pre-trained.keras')\n",
    "\n",
    "# # Load the model\n",
    "# loaded_model = load_model('path_to_my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d158fbb7",
   "metadata": {},
   "source": [
    "## Re-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290fa9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embedding_dim, weights=[embedding_matrix]))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, train_labels, epochs=2, batch_size=300, validation_split=0.1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "model.save('glove-6B-300d-Re-trained.keras')\n",
    "\n",
    "# # Load the model\n",
    "# loaded_model = load_model('path_to_my_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
